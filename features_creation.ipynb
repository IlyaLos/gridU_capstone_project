{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from gensim.models import *\n",
    "import string\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from scipy.stats.stats import pearsonr\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "printable = set(string.printable)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "alphanum = []\n",
    "for x in printable:\n",
    "    if x.isalnum():\n",
    "        alphanum.append(x)\n",
    "        \n",
    "alphanum = set(alphanum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read dataframes\n",
    "attributes = pd.read_csv('data/attributes.csv')\n",
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')\n",
    "description = pd.read_csv('data/product_descriptions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete columns\n",
    "train.drop('Unnamed: 0', 1, inplace=True)\n",
    "description.drop('Unnamed: 0', 1, inplace=True)\n",
    "\n",
    "# fill na in attributes dataframe\n",
    "attributes = attributes.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "# Find all brands in attributes\n",
    "brand = attributes[ attributes.name == 'MFG Brand Name' ]\n",
    "brand.columns = ['product_uid', 'name', 'brand_name']\n",
    "\n",
    "brand.drop('name', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"\nC:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  import sys\n"
     ]
    }
   ],
   "source": [
    "# Find all colors in attributes\n",
    "color = attributes[ (attributes.name == 'Color/Finish') | (attributes.name == 'Color') ]\n",
    "color.columns = ['product_uid', 'name', 'color']\n",
    "\n",
    "color.drop('name', 1, inplace = True)\n",
    "\n",
    "color.drop_duplicates(['product_uid'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  \"\"\"\nC:\\Anaconda2\\lib\\site-packages\\ipykernel_launcher.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n  import sys\n"
     ]
    }
   ],
   "source": [
    "# Find all materials in attributes\n",
    "material = attributes[ attributes.name == 'Material' ]\n",
    "material.columns = ['product_uid', 'name', 'material']\n",
    "\n",
    "material.drop('name', 1, inplace = True)\n",
    "\n",
    "material.drop_duplicates(['product_uid'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train and test with product descriptions\n",
    "train = train.set_index('product_uid').join(description.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index()\n",
    "test = test.set_index('product_uid').join(description.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index()\n",
    "\n",
    "# Merge train and test with brand\n",
    "train = train.set_index('product_uid').join(brand.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index()\n",
    "test = test.set_index('product_uid').join(brand.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index()\n",
    "\n",
    "# Merge train and test with color\n",
    "train = train.set_index('product_uid').join(color.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index()\n",
    "test = test.set_index('product_uid').join(color.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index()\n",
    "\n",
    "# Merge train and test with material\n",
    "train = train.set_index('product_uid').join(material.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index().set_index('id')\n",
    "test = test.set_index('product_uid').join(material.set_index('product_uid'), how = 'left')\\\n",
    "    .reset_index().set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill NaN with \"\"\n",
    "train = train.fillna(\"\")\n",
    "test = test.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc minimum distance between string text2 and substring of text1\n",
    "# dynamic programming\n",
    "# delete one char costs 1\n",
    "# insert one char costs 1\n",
    "# change one char costs 1\n",
    "# return value in range [0, 1]\n",
    "# if return 0 then string text1 apears in text2\n",
    "# if return 1 then any chars from text1 not apears in text2\n",
    "def get_dist(text1, text2):\n",
    "    d = [np.ones(len(text2) + 1) * int(1e9) for i in range(len(text1) + 1)]\n",
    "    for i in range(len(text1) + 1):\n",
    "        d[i][0] = 0\n",
    "        \n",
    "    for i in range(len(text1) + 1):\n",
    "        for j in range(len(text2) + 1):\n",
    "            dv = d[i][j]\n",
    "            if dv == int(1e9):\n",
    "                continue\n",
    "            \n",
    "            if i < len(text1) and j < len(text2):\n",
    "                d[i + 1][j + 1] = min(d[i + 1][j + 1], dv + int(text1[i] != text2[j]))\n",
    "            if i < len(text1):\n",
    "                d[i + 1][j] = min(d[i + 1][j], dv + 1)\n",
    "            if j < len(text2):\n",
    "                d[i][j + 1] = min(d[i][j + 1], dv + 1)\n",
    "                    \n",
    "    result = len(text2)\n",
    "    for i in range(len(text1) + 1):\n",
    "        result = min(result, d[i][len(text2)])\n",
    "        \n",
    "    return float(result) / len(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86263\n"
     ]
    }
   ],
   "source": [
    "# create map <product_uid, concatenating of all bulletXX attributes for this product_uid>\n",
    "bullet = {}\n",
    "\n",
    "for i in range(attributes.shape[0]):\n",
    "    id = attributes.index[i]\n",
    "    \n",
    "    if str(attributes['name'][id]) == \"\":\n",
    "        continue\n",
    "    \n",
    "    if str(attributes['name'][id])[0:6] != \"Bullet\":\n",
    "        continue\n",
    "        \n",
    "    pid = attributes['product_uid'][id]\n",
    "    \n",
    "    bullet[pid] = bullet.get(pid, \"\") +  \" \" + str(attributes['value'][id])\n",
    "    \n",
    "print len(bullet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string to words list\n",
    "# delete all non-ascii symbols\n",
    "# convert all alpha to lower case\n",
    "# tokenize with nltk.word_tokenize()\n",
    "# stem each word with snowball stemmer\n",
    "def string_to_normalized_word_list(s):\n",
    "    s = str(filter(lambda x: x in printable, s)).lower()\n",
    "    return [stemmer.stem(word) for word in nltk.word_tokenize(s)]\n",
    "\n",
    "# delete all non-alphanumeric symbols\n",
    "def filter_string_alphanum_only(s):\n",
    "    return str(filter(lambda x: x in alphanum, s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc feature for brand_name with function get_dist()\n",
    "def calc_brand_feature(data):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        text = filter_string_alphanum_only(data.iloc[i]['brand_name']).lower()\n",
    "        query = filter_string_alphanum_only(data.iloc[i]['search_term']).lower()\n",
    "\n",
    "        if text == \"\":\n",
    "            result.append(0)\n",
    "        else:\n",
    "            cur = get_dist(query, text)\n",
    "            result.append(cur)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc feature for product_title with function get_dist()\n",
    "def calc_title_feature(data):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        text = filter_string_alphanum_only(data.iloc[i][\"product_title\"]).lower()\n",
    "        query = filter_string_alphanum_only(data.iloc[i]['search_term']).lower()\n",
    "\n",
    "        if text == \"\":\n",
    "            result.append(0)\n",
    "        else:\n",
    "            cur = get_dist(query, text)\n",
    "            result.append(cur)\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc two features for 'param_name'\n",
    "# for each row in dataframe data: get parameters 'param_name' and 'search_term', normalized them, deleted stop-words \n",
    "# and join words of parameter 'param_name' by space\n",
    "# then finding occurrences of each word (1-gram) of 'search_term' in string of 'param_name' (result1)\n",
    "#                      and of each pair of adjacent words (bi-gram) of 'search_term' in string of 'param_name' (result2)\n",
    "# and finally each values divide by number of words in 'search_term'\n",
    "# so we calc ratio of words (and pairs of adjacent words) that appears in 'param_name' as substrings\n",
    "def calc_features(data, param_name):\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        text = \" \".join(filter(lambda word: word not in stop_words, string_to_normalized_word_list(data.iloc[i][param_name])))\n",
    "        query = filter(lambda word: word not in stop_words, string_to_normalized_word_list(data.iloc[i]['search_term']))\n",
    "        \n",
    "        if len(query) == 0:\n",
    "            result1.append(0)\n",
    "        else:        \n",
    "            cnt1 = 0\n",
    "            for word in query:\n",
    "                if text.find(word) != -1:\n",
    "                    cnt1 += 1\n",
    "                        \n",
    "            result1.append(float(cnt1) / len(query))\n",
    "        \n",
    "        if len(query) <= 1:\n",
    "            result2.append(0)\n",
    "        else:\n",
    "            cnt2 = 0\n",
    "            for j in range(len(query) - 1):\n",
    "                if text.find(query[j] + \" \" + query[j + 1]) != -1:\n",
    "                    cnt2 += 1\n",
    "                        \n",
    "            result2.append(float(cnt2) / (len(query) - 1))\n",
    "\n",
    "    return result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all as in function calc_features(), but only for bullet attributes instead of 'param_name'\n",
    "def calc_bullet_features(data):\n",
    "    result1 = []\n",
    "    result2 = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        text = \" \".join(filter(lambda word: word not in stop_words, \n",
    "                               string_to_normalized_word_list(bullet.get(data.iloc[i]['product_uid'], \"\"))))\n",
    "        query = filter(lambda word: word not in stop_words, string_to_normalized_word_list(data.iloc[i]['search_term']))\n",
    "        \n",
    "        if len(query) == 0:\n",
    "            result1.append(0)\n",
    "        else:        \n",
    "            cnt1 = 0\n",
    "            for word in query:\n",
    "                for word2 in text:\n",
    "                    if word == word2:\n",
    "                        cnt1 += 1\n",
    "                        break\n",
    "                        \n",
    "            result1.append(float(cnt1) / len(query))\n",
    "        \n",
    "        if len(query) <= 1:\n",
    "            result2.append(0)\n",
    "        else:\n",
    "            cnt2 = 0\n",
    "            for j in range(len(query) - 1):\n",
    "                for k in range(len(text) - 1):\n",
    "                    if query[j] == text[k] and query[j + 1] == text[k + 1]:\n",
    "                        cnt2 += 1\n",
    "                        break\n",
    "                        \n",
    "            result2.append(float(cnt2) / (len(query) - 1))\n",
    "\n",
    "    return result1, result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc dataframe of features for dataframe data and list param_names of 'param_name's\n",
    "def calc_X (data, param_names):\n",
    "    frame_dict = {'ones': np.ones(len(data)).tolist()} # add column with 1s for linear models\n",
    "    for name in param_names:\n",
    "        cur = calc_features(data, name)\n",
    "        id = 1\n",
    "        for vec in cur:\n",
    "            frame_dict[name + \"_\" + str(id)] = vec\n",
    "            id += 1\n",
    "\n",
    "    return pd.DataFrame(frame_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataframes with count features normalized by count of words for parameters 'brand_name', \n",
    "# 'product_title', 'product_description', 'color', 'material'\n",
    "X_train = calc_X(train, [\"brand_name\", \"product_title\", \"product_description\", \"color\", \"material\"])\n",
    "X_test = calc_X(test, [\"brand_name\", \"product_title\", \"product_description\", \"color\", \"material\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features for 'brand_name' and 'product_title' with function get_dist()\n",
    "X_train['brand'] = calc_brand_feature(train)\n",
    "X_test['brand'] = calc_brand_feature(test)\n",
    "\n",
    "X_train['title'] = calc_title_feature(train)\n",
    "X_test['title'] = calc_title_feature(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating features for 'bullet' attributes\n",
    "cur = calc_bullet_features(train)\n",
    "X_train['bullet_1'] = cur[0]\n",
    "X_train['bullet_2'] = cur[1]\n",
    "\n",
    "cur = calc_bullet_features(test)\n",
    "X_test['bullet_1'] = cur[0]\n",
    "X_test['bullet_2'] = cur[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating boolean features for 'brand_name', 'color', 'material' and 'bullet' attributes\n",
    "# 1 if we have that value for that product, 0 - otherwise\n",
    "b1_train = []\n",
    "b2_train = []\n",
    "b3_train = []\n",
    "b4_train = []\n",
    "\n",
    "for i in range(len(train)):\n",
    "    b1_train.append(int(train.iloc[i]['brand_name'] == \"\"))\n",
    "    b2_train.append(int(train.iloc[i]['color'] == \"\"))\n",
    "    b3_train.append(int(train.iloc[i]['material'] == \"\"))\n",
    "    b4_train.append(int(bullet.get(train.iloc[i]['product_uid'], \"\") == \"\"))\n",
    "    \n",
    "    \n",
    "b1_test = []\n",
    "b2_test = []\n",
    "b3_test = []\n",
    "b4_test = []\n",
    "\n",
    "for i in range(len(test)):\n",
    "    b1_test.append(int(test.iloc[i]['brand_name'] == \"\"))\n",
    "    b2_test.append(int(test.iloc[i]['color'] == \"\"))\n",
    "    b3_test.append(int(test.iloc[i]['material'] == \"\"))\n",
    "    b4_test.append(int(bullet.get(test.iloc[i]['product_uid'], \"\") == \"\"))\n",
    "    \n",
    "    \n",
    "X_train['b1'] = b1_train\n",
    "X_train['b2'] = b2_train\n",
    "X_train['b3'] = b3_train\n",
    "X_train['b4'] = b4_train\n",
    "\n",
    "X_test['b1'] = b1_test\n",
    "X_test['b2'] = b2_test\n",
    "X_test['b3'] = b3_test\n",
    "X_test['b4'] = b4_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating word2vec model from 'product_description' with 100 features\n",
    "\n",
    "documents = [nltk.word_tokenize(text.lower()) for text in description['product_description'].tolist()]# + \\\n",
    "            #[nltk.word_tokenize(text.lower()) for text in train['search_term'].tolist()] + \\\n",
    "            #[nltk.word_tokenize(text.lower()) for text in test['search_term'].tolist()]\n",
    "\n",
    "word2vec_model = Word2Vec(documents, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc average vector of word2vec vectors for words from list words\n",
    "def sum_vector(words):\n",
    "    res = np.zeros(100)\n",
    "    cnt = 0\n",
    "    for word in words:\n",
    "        if word in word2vec_model.wv:\n",
    "            res += np.array(word2vec_model.wv[word])\n",
    "            cnt += 1\n",
    "            \n",
    "    if cnt > 0:\n",
    "        res /= cnt\n",
    "            \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc cosine similarity between two vectors \n",
    "def similarity(vec1, vec2):\n",
    "    res = 0\n",
    "    len1 = 0\n",
    "    len2 = 0\n",
    "    for i in range(len(vec1)):\n",
    "        res += vec1[i] * vec2[i]\n",
    "        len1 += vec1[i] ** 2\n",
    "        len2 += vec2[i] ** 2\n",
    "        \n",
    "    if len1 == 0 or len2 == 0:\n",
    "        return 1\n",
    "        \n",
    "    res /= math.sqrt(len1)\n",
    "    res /= math.sqrt(len2)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc feature: cosine similarity between average word2vec vectors of words 'param_name' and 'search_term' \n",
    "def calc_word2vec_features(data, param_name):\n",
    "    result = []\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        text = string_to_normalized_word_list(data.iloc[i][param_name])\n",
    "        query = string_to_normalized_word_list(data.iloc[i]['search_term'])\n",
    "        \n",
    "        result.append(similarity(sum_vector(text), sum_vector(query)))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc word2vec cosine similarity functions for 'product_descriptions'\n",
    "X_train['word2vec_description'] = calc_word2vec_features(train, \"product_description\")\n",
    "X_test['word2vec_description'] = calc_word2vec_features(test, \"product_description\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf vectorizer for 'product_description'\n",
    "# normalize 'product_description' at first\n",
    "docs = map(lambda x: \" \".join(string_to_normalized_word_list(x)), \n",
    "           description['product_description'].tolist())\n",
    "\n",
    "# analyze by words, tokenize with nltk.word_tokenize(), deleting stop-words, all alpha to lower case \n",
    "tf_idf = TfidfVectorizer(docs, analyzer='word', tokenizer=nltk.word_tokenize, stop_words='english', lowercase=True)\n",
    "\n",
    "tf_idf_description = tf_idf.fit_transform(docs)\n",
    "tf_idf_X_train = tf_idf.transform(map(lambda x: \" \".join(string_to_normalized_word_list(x)),\n",
    "                                      train['search_term'].tolist()))\n",
    "tf_idf_X_test = tf_idf.transform(map(lambda x: \" \".join(string_to_normalized_word_list(x)),\n",
    "                                     test['search_term'].tolist()))\n",
    "\n",
    "\n",
    "desc_map = {}\n",
    "for i in range(description.shape[0]):\n",
    "    desc_map[ description.iloc[i]['product_uid'] ] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf_idf features \n",
    "\n",
    "tf_idf_train = []\n",
    "for i in range(train.shape[0]):\n",
    "    pid = train.iloc[i]['product_uid']\n",
    "    tf_idf_train.append(cosine_similarity(tf_idf_X_train[i], tf_idf_description[ desc_map[pid] ])[0][0])\n",
    "     \n",
    "X_train['tf_idf'] = tf_idf_train\n",
    "\n",
    "\n",
    "tf_idf_test = []\n",
    "for i in range(test.shape[0]):\n",
    "    pid = test.iloc[i]['product_uid']\n",
    "    tf_idf_test.append(cosine_similarity(tf_idf_X_test[i], tf_idf_description[ desc_map[pid] ])[0][0])\n",
    "     \n",
    "X_test['tf_idf'] = tf_idf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv('X_train.csv')\n",
    "X_test.to_csv('X_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}